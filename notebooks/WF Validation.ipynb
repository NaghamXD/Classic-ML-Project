{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "#for models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "#for PCA & PLS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "#for Data\n",
    "import yfinance as yf\n",
    "\n",
    "#for Data Distribution\n",
    "from scipy.stats import kurtosis, skew, shapiro\n",
    "from scipy import stats\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch historical data for the S&P 500\n",
    "sp500_data = yf.download('^GSPC', start='2002-01-01', end='2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sp500_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the data\n",
    "nan_values_df = sp500_data.isna().any()\n",
    "\n",
    "print(\"NaN values in DataFrame:\")\n",
    "print(nan_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Moving Average:\n",
    "#returns the dataframe with additional coumn of simple moving average\n",
    "def calculate_sma(df: pd.DataFrame, column: str = 'Adj Close', window_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Simple Moving Average (SMA) for a given column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the financial data.\n",
    "        column (str): Name of the column for which to calculate SMA. Default is 'close'.\n",
    "        window_size (int): Size of the moving window. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with SMA column added.\n",
    "    \"\"\"\n",
    "    # Calculate SMA\n",
    "    sma = df[column].rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    # Create a DataFrame to store SMA\n",
    "    df['SMA'] = sma\n",
    "    df['SMA_signal'] = df['Close'] - df['SMA']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted Moving Average\n",
    "def calculate_wma(df: pd.DataFrame, column: str = 'Adj Close', window_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Weighted Moving Average (WMA) for a given column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the financial data.\n",
    "        column (str): Name of the column for which to calculate WMA. Default is 'close'.\n",
    "        window_size (int): Size of the moving window. Default is 15.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with WMA and WMA signal columns added.\n",
    "    \"\"\"\n",
    "    # Generate the weights\n",
    "    weights = np.arange(1, window_size + 1)\n",
    "    data = df[column]\n",
    "    \n",
    "    # Calculate the WMA using convolution\n",
    "    wma = data.rolling(window=window_size).apply(lambda prices: np.dot(prices, weights) / weights.sum(), raw=True)\n",
    "    \n",
    "    # Create a DataFrame to store WMA\n",
    "    df['WMA'] = wma\n",
    "    \n",
    "    # Add WMA signal column\n",
    "    df['WMA_signal'] = df[column] - wma\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACD\n",
    "def calculate_macd(df: pd.DataFrame, short_window:int=12, long_window:int=26, signal_window:int=9, column: str = 'Adj Close') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Moving Average Convergence Divergence (MACD) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        short_window (int): The short-term window size for the short EMA.\n",
    "        long_window (int): The long-term window size for the long EMA.\n",
    "        signal_window (int): The window size for the signal line EMA.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for MACD and signal line.\n",
    "    \"\"\"\n",
    "    # Calculate short-term EMA\n",
    "    short_ema = df[column].ewm(span=short_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Calculate long-term EMA\n",
    "    long_ema = df[column].ewm(span=long_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Calculate MACD line\n",
    "    macd_line = short_ema - long_ema\n",
    "    \n",
    "    # Calculate signal line\n",
    "    signal_line = macd_line.ewm(span=signal_window, min_periods=1, adjust=False).mean()\n",
    "    \n",
    "    # Store MACD and signal line in the DataFrame\n",
    "    df['MACD'] = macd_line\n",
    "    df['Signal Line'] = signal_line\n",
    "    df['macd_signal'] = macd_line - signal_line\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic_oscillator\n",
    "def calculate_stochastic_oscillator(df, k_fast_period=14, k_slow_period=3, d_slow_period=3, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Stochastic Oscillator and its corresponding moving averages (K and D lines).\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        k_fast_period (int): The period for the fast %K line.\n",
    "        k_slow_period (int): The period for the slow %K line.\n",
    "        d_slow_period (int): The period for the slow %D line.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for %K_fast, %K_slow, %D_fast, and %D_slow.\n",
    "    \"\"\"\n",
    "    # Calculate highest high and lowest low over the period\n",
    "    HH = df['High'].rolling(window=k_fast_period).max()\n",
    "    LL = df['Low'].rolling(window=k_fast_period).min()\n",
    "\n",
    "    # Calculate %K_fast\n",
    "    df['%K_fast'] = ((df[column] - LL) / \n",
    "                     (HH - LL)) * 100\n",
    "    \n",
    "    # Calculate %K_slow (smoothed %K_fast)\n",
    "    df['%K_slow'] = df['%K_fast'].rolling(window=k_slow_period).mean()\n",
    "    \n",
    "    # Calculate %D_fast (3-day SMA of %K_slow)\n",
    "    df['%D_fast'] = df['%K_slow'].rolling(window=d_slow_period).mean()\n",
    "    \n",
    "    # Calculate %D_slow (3-day SMA of %D_fast)\n",
    "    df['%D_slow'] = df['%D_fast'].rolling(window=d_slow_period).mean()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSI\n",
    "def calculate_rsi(df, window_size=14, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for calculating RSI.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional column for RSI.\n",
    "    \"\"\"\n",
    "    # Calculate price changes\n",
    "    delta = df[column].diff()\n",
    "    \n",
    "    # Define up and down moves\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window_size).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window_size).mean()\n",
    "    \n",
    "    # Calculate the relative strength (RS)\n",
    "    rs = gain / loss\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Store RSI in the DataFrame\n",
    "    df['RSI'] = rsi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WPR\n",
    "def calculate_williams_percent_r(df, window=14, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Williams %R (WPR) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for calculating WPR.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with an additional column for WPR.\n",
    "    \"\"\"\n",
    "    # Calculate highest high and lowest low over the window\n",
    "    highest_high = df['High'].rolling(window=window).max()\n",
    "    lowest_low = df['Low'].rolling(window=window).min()\n",
    "    \n",
    "    # Calculate Williams %R\n",
    "    wpr = ((highest_high - df[column]) / (highest_high - lowest_low)) * -100\n",
    "    \n",
    "    # Store WPR in the DataFrame\n",
    "    df['WPR'] = wpr\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bollinger Bands\n",
    "def calculate_bollinger_bands(df, window=20, num_std_dev=2, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        window (int): The window size for the moving average.\n",
    "        num_std_dev (int): The number of standard deviations for the bands.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional columns for Bollinger Bands.\n",
    "    \"\"\"\n",
    "    # Calculate rolling mean and standard deviation\n",
    "    rolling_mean = df[column].rolling(window=window).mean()\n",
    "    rolling_std = df[column].rolling(window=window).std()\n",
    "    \n",
    "    # Calculate upper and lower bands\n",
    "    upper_band = rolling_mean + (rolling_std * num_std_dev)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std_dev)\n",
    "    \n",
    "    # Store Bollinger Bands in the DataFrame\n",
    "    df['Bollinger Upper'] = upper_band\n",
    "    df['Bollinger Lower'] = lower_band\n",
    "    df['Bollinger Diff'] = upper_band - lower_band\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On-Balance Volume (OBV)\n",
    "def calculate_obv(df, column: str = 'Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate On-Balance Volume (OBV) for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional column for OBV.\n",
    "    \"\"\"\n",
    "    obv_values = []\n",
    "    prev_obv = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if df[column].iloc[i] > df[column].iloc[i - 1]:\n",
    "            obv = prev_obv + df['Volume'].iloc[i]\n",
    "        elif df[column].iloc[i] < df[column].iloc[i - 1]:\n",
    "            obv = prev_obv - df['Volume'].iloc[i]\n",
    "        else:\n",
    "            obv = prev_obv\n",
    "\n",
    "        obv_values.append(obv)\n",
    "        prev_obv = obv\n",
    "\n",
    "    # Add initial OBV value as 0\n",
    "    obv_values = [0] + obv_values\n",
    "\n",
    "    # Store OBV in the DataFrame\n",
    "    df['OBV'] = obv_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average True Range (ATR)\n",
    "def calculate_atr(df, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Average True Range (ATR) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', and 'Close' columns representing high, low, and closing prices respectively.\n",
    "        period (int): Number of periods for which to calculate the ATR (default is 14).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'ATR' column containing the calculated ATR values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    \n",
    "    # True Range (TR) calculation\n",
    "    df['TR'] = df[['High', 'Low', 'Adj Close']].apply(lambda row: max(row['High'] - row['Low'], abs(row['High'] - row['Adj Close']), abs(row['Low'] - row['Adj Close'])), axis=1)\n",
    "    \n",
    "    # ATR calculation\n",
    "    df['ATR'] = df['TR'].rolling(period).mean()\n",
    "    \n",
    "    # Drop the TR column if not needed\n",
    "    df.drop('TR', axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rice Rate of Change (ROC)\n",
    "def calculate_roc(df, n_periods=12, column='Adj Close'):\n",
    "    \"\"\"\n",
    "    Calculate the Price Rate of Change (ROC) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'Adj Close' column representing closing prices.\n",
    "        n_periods (int): Number of periods for which to calculate the ROC. # It can be anything such as 12, 25,\n",
    "        or 200. Short-term trader traders typically use a smaller number while longer-term investors use a larger\n",
    "        number.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'ROC' column containing the calculated ROC values.\n",
    "    \"\"\"\n",
    "    close_prices = df[column]\n",
    "    close_prices_shifted = close_prices.shift(n_periods)\n",
    "    \n",
    "    roc = ((close_prices - close_prices_shifted) / close_prices_shifted) * 100\n",
    "    \n",
    "    df['ROC'] = roc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Money Flow Index - MFI\n",
    "def calculate_mfi(df, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Money Flow Index (MFI) of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', 'Close', and 'Volume' columns representing high, low, closing prices, and volume respectively.\n",
    "        period (int): Number of periods for which to calculate the MFI (default is 14).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'MFI' column containing the calculated MFI values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    volume = df['Volume']\n",
    "    \n",
    "    # Typical Price calculation\n",
    "    tp = (high + low + close) / 3\n",
    "    \n",
    "    # Raw Money Flow calculation\n",
    "    mf = tp * volume\n",
    "    \n",
    "    # Determine whether the typical price is higher or lower than the previous period\n",
    "    tp_shifted = tp.shift(1)\n",
    "    positive_flow = (tp > tp_shifted)\n",
    "    negative_flow = (tp < tp_shifted)\n",
    "    \n",
    "    # Calculate positive and negative money flow\n",
    "    positive_mf = positive_flow * mf\n",
    "    negative_mf = negative_flow * mf\n",
    "    \n",
    "    # Calculate the Money Flow Ratio (MFR)\n",
    "    mfr = positive_mf.rolling(window=period).sum() / negative_mf.rolling(window=period).sum()\n",
    "    \n",
    "    # Calculate the Money Flow Index (MFI)\n",
    "    mfi = 100 - (100 / (1 + mfr))\n",
    "    \n",
    "    df['MFI'] = mfi\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaikin_oscillator\n",
    "def calculate_chaikin_oscillator(df, short_period=3, long_period=10):\n",
    "    \"\"\"\n",
    "    Calculate the Chaikin Oscillator of a stock dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing stock data, with 'High', 'Low', 'Adj Close', and 'Volume' columns representing high, low, closing prices, and volume respectively.\n",
    "        short_period (int): Number of periods for the short EMA (default is 3).\n",
    "        long_period (int): Number of periods for the long EMA (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with 'Chaikin_Oscillator' column containing the calculated Chaikin Oscillator values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Adj Close']\n",
    "    volume = df['Volume']\n",
    "    \n",
    "    # Money Flow Multiplier calculation\n",
    "    mfm = ((close - low) - (high - close)) / (high - low)\n",
    "    \n",
    "    # Money Flow Volume calculation\n",
    "    mfv = mfm * volume\n",
    "    \n",
    "    # Accumulation/Distribution Line (ADL) calculation\n",
    "    adl = mfv.cumsum()\n",
    "    \n",
    "    # Calculate the EMA for ADL\n",
    "    ema_short = adl.ewm(span=short_period, min_periods=short_period, adjust=False).mean()\n",
    "    ema_long = adl.ewm(span=long_period, min_periods=long_period, adjust=False).mean()\n",
    "    \n",
    "    # Calculate the Chaikin Oscillator\n",
    "    chaikin_oscillator = ema_short - ema_long\n",
    "    \n",
    "    df['Chaikin_Oscillator'] = chaikin_oscillator\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bulid the technical indicators: features\n",
    "def technical_indicators(df):\n",
    "    df = calculate_sma(df)\n",
    "    df = calculate_wma(df)\n",
    "    df = calculate_macd(df)\n",
    "    df = calculate_rsi(df)\n",
    "    df = calculate_stochastic_oscillator(df)\n",
    "    df = calculate_bollinger_bands(df)\n",
    "    df = calculate_williams_percent_r(df)\n",
    "    df = calculate_obv(df)\n",
    "    df = calculate_roc(df)\n",
    "    df = calculate_atr(df)\n",
    "    df = calculate_mfi(df)\n",
    "    df = calculate_chaikin_oscillator(df)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All analyzed technical indicators are lagged by one period before being used as predictors for returns in the models in order to avoid the so-called look ahead \n",
    "# bias involving making decisions in the same period for which the given signal was generated.\n",
    "def lag_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Lag all columns in a DataFrame by one period.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the calculated technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with all columns lagged by one period.\n",
    "    \"\"\"\n",
    "    # Lag all columns by one period\n",
    "    df_lagged = df.shift()\n",
    "    \n",
    "    return df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Function\n",
    "# Technical analysis indicators need to be rescaled before being fed to the models.\n",
    "# The process is conducted using a version of min-max normalization technique which produces outputs in range from ‐1 to 1.\n",
    "# This technique was chosen for two reasons: it is intuitive as the machine learning models produce output \n",
    "# variable that is also ranging from ‐1 to 1 and because it causes the input data to be more comparable. \n",
    "# X'(t) = (X(t) - min(x)) / (max(x) - min(x))*2 -1\n",
    "\n",
    "def feature_transform(df):\n",
    "    \"\"\"\n",
    "    Transform all columns in the DataFrame as the following formula\n",
    "    X'(t) = (X(t) - min(x)) / (max(x) - min(x))*2 -1\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the calculated technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with all columns transformed.\n",
    "    \"\"\"\n",
    "    max_x = df.max()\n",
    "    min_x = df.min()\n",
    "\n",
    "    df_transformed = (df - min_x) / (max_x - min_x) * 2 - 1\n",
    "\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#truncate the dataframe from the biggining so the walk forward splits will continue untill the last date\n",
    "def truncate_before_wf(df, in_sample_size, out_sample_size):\n",
    "    drop_index = (len(df) - in_sample_size) % out_sample_size\n",
    "    return (df.iloc[drop_index:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research employed dynamic estimation windows which means that the underlying parameters of the models \n",
    "# were periodically recalibrated to reflect current market behaviors. Observations from the beginning \n",
    "# of the available period were initially trimmed in order for the overall number of observations for\n",
    "# each index to be easily divisible into equal subsets. Calibration of models’ parameters was conducted \n",
    "# on 200 trading day window (in-sample) and then model predictions were applied onto next 20 trading day\n",
    "# window (out-of-sample). For each subsequent dynamic window iteration, in-sample and out-of-sample moved \n",
    "# by 20 trading days. \n",
    "\n",
    "def walk_forward_validation(df, in_sample_size, out_sample_size):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation on a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        in_sample_size (int): Number of periods to use for in-sample data.\n",
    "        out_sample_size (int): Number of periods to use for out-of-sample data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Tuple containing lists of in-sample and out-of-sample data.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    n_subsets = (total_rows - in_sample_size) // out_sample_size\n",
    "    splits = []\n",
    "        \n",
    "    for i in range(n_subsets):\n",
    "        start_index = i * out_sample_size\n",
    "        end_index = start_index + in_sample_size + out_sample_size\n",
    "        \n",
    "        if end_index > total_rows:\n",
    "            break\n",
    "        \n",
    "        in_sample = df.iloc[start_index : start_index + in_sample_size]\n",
    "        out_of_sample = df.iloc[start_index + in_sample_size : end_index]\n",
    "        \n",
    "        splits.append((in_sample, out_of_sample))\n",
    "    return (splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation coefficients between each feature and the return & print it\n",
    "\n",
    "def correlation(df, target_name):\n",
    "\n",
    "    correlation_with_target = np.abs(df.corrwith(df[target_name]))\n",
    "\n",
    "    # Display the correlation coefficients\n",
    "    print(\"Correlation with Log return:\")\n",
    "    print(correlation_with_target.sort_values(ascending=False))\n",
    "    correlation_with_target.sort_values().plot.barh(color = 'blue',title = 'Strength of Correlation', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus far, we've only used a simple correlation statistic across the full time period. \n",
    "#This is a good place to start but, is a dangerous place to stop. Financial time series data suffers\n",
    "# from non-stationarity and regime change, so a relationship which on average has existed may have been \n",
    "#wildly unstable over time.\n",
    "\n",
    "#To check, we'll plot the rolling correlation of these selected features.\n",
    "\n",
    "# Compute the rolling correlation for each pair of selected features\n",
    "def rolling_correlation(df, target_name, window_size = 200):\n",
    "\n",
    "    correlation_with_target_200 = df.rolling(window=window_size).corr(df[target_name])\n",
    "    # Create traces for each feature\n",
    "    traces = []\n",
    "    for feature in df.columns:\n",
    "        trace = go.Scatter(\n",
    "            x=correlation_with_target_200.index,\n",
    "            y=correlation_with_target_200[feature],\n",
    "            mode='lines',\n",
    "            name=feature\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Create layout for the plot\n",
    "        layout = go.Layout(\n",
    "        title='Rolling Correlation of Features with Log Return',\n",
    "        xaxis=dict(title='Index'),\n",
    "        yaxis=dict(title='Rolling Correlation with Log Return'),\n",
    "        hovermode='closest',\n",
    "        autosize=True\n",
    "    )\n",
    "\n",
    "    # Create figure object\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pair plot\n",
    "def features_paiplot(df):\n",
    "    pairplot = sns.pairplot(df, height=1.5)\n",
    "\n",
    "    # Set the title\n",
    "    pairplot.figure.suptitle('Pairplot of features', y=1.02)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers_iqr_df(df, k=1.5):\n",
    "    \"\"\"\n",
    "    Count the number of outliers in each column of the DataFrame using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - k: The multiplier for the IQR. Typically set to 1.5 to 3.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary where keys are column names and values are the number of outliers detected in each column.\n",
    "    \"\"\"\n",
    "    outliers_counts = {}\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        quartile_1, quartile_3 = np.percentile(data, [25, 75])\n",
    "        iqr = quartile_3 - quartile_1\n",
    "        lower_bound = quartile_1 - (k * iqr)\n",
    "        upper_bound = quartile_3 + (k * iqr)\n",
    "        outliers = (data < lower_bound) | (data > upper_bound)\n",
    "        outliers_counts[col] = np.sum(outliers)\n",
    "    return outliers_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncate NaN Data\n",
    "def drop_nan(df):\n",
    "    # Remove rows with NaN values\n",
    "    cleaned_df = df.dropna()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check distribution \n",
    "def check_distribution(df, column_name='Adj Close'):\n",
    "    \"\"\"\n",
    "    Check the distribution of a column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing the data.\n",
    "        column_name (str): Name of the column to check the distribution for (default is 'Adj Close').\n",
    "\n",
    "    Returns:\n",
    "        None (displays descriptive statistics and visualizations)\n",
    "    \"\"\"\n",
    "    # Descriptive statistics\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    print(df[column_name].describe())\n",
    "\n",
    "    # Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column_name], kde=True)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_normal(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test if the data is normally distributed using Z-score.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The input data array.\n",
    "    - alpha: The significance level for the test.\n",
    "    \n",
    "    Returns:\n",
    "    - True if the data is normally distributed, False otherwise.\n",
    "    \"\"\"\n",
    "    normal_col = {}\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        z_score, p_value = stats.normaltest(data)\n",
    "        normal_col[col] = p_value > alpha\n",
    "    return normal_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Percentage Error (MAPE)\n",
    "    \n",
    "    Args:\n",
    "    actual: array-like, actual values\n",
    "    predicted: array-like, predicted values\n",
    "    \n",
    "    Returns:\n",
    "    mape: float, MAPE value\n",
    "    \"\"\"\n",
    "    # Ensure both actual and predicted arrays have the same length\n",
    "    if len(actual) != len(predicted):\n",
    "        raise ValueError(\"Length of actual and predicted arrays must be the same.\")\n",
    "    \n",
    "\n",
    "    # Calculate absolute percentage error for each observation\n",
    "    abs_percentage_error = np.abs((actual - predicted) / np.maximum(np.abs(actual), 1e-10))\n",
    "    \n",
    "    # Calculate mean of absolute percentage errors\n",
    "    mape = np.mean(abs_percentage_error) * 100\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_series, predicted_series):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of a model that predicts buy, hold, or sell signals.\n",
    "\n",
    "    Parameters:\n",
    "    true_series (Series): Pandas Series containing true labels.\n",
    "    predicted_series (Series): Pandas Series containing predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (float): Accuracy of the model.\n",
    "    \"\"\"\n",
    "    if len(true_series) != len(predicted_series):\n",
    "        raise ValueError(\"The lengths of true_series and predicted_series must be equal.\")\n",
    "\n",
    "    correct_predictions = sum(1 for true_label, predicted_label in zip(true_series,\n",
    "                             predicted_series) if true_label == predicted_label)\n",
    "    total_predictions = len(true_series)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe_ratio(returns, risk_free_rate=0):\n",
    "    \"\"\"\n",
    "    Calculate the Sharpe Ratio.\n",
    "\n",
    "    Parameters:\n",
    "    returns (Series or DataFrame): Series or DataFrame containing the returns.\n",
    "    risk_free_rate (float, optional): Risk-free rate of return. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    float or Series: Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    # Calculate excess returns over the risk-free rate\n",
    "    excess_returns = returns - risk_free_rate\n",
    "\n",
    "    # Calculate the annualized Sharpe Ratio\n",
    "    sharpe_ratio = np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "    return sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ir(stock_returns, benchmark_returns):\n",
    "    \"\"\"\n",
    "    Calculate the Information Ratio (IR).\n",
    "\n",
    "    Parameters:\n",
    "    active_returns (Series or DataFrame): Series or DataFrame containing the active returns (excess returns over benchmark).\n",
    "    benchmark_returns (Series or DataFrame): Series or DataFrame containing the benchmark returns.\n",
    "\n",
    "    Returns:\n",
    "    float or Series: Information Ratio.\n",
    "    \"\"\"\n",
    "    # Calculate excess returns\n",
    "    excess_returns = stock_returns - benchmark_returns\n",
    "\n",
    "    # Calculate average excess return\n",
    "    avg_excess_return = np.mean(excess_returns)\n",
    "    \n",
    "    # Calculate standard deviation of excess returns\n",
    "    std_excess_return = np.std(excess_returns)\n",
    "    \n",
    "    # Calculate Information Ratio\n",
    "    ir = avg_excess_return / std_excess_return\n",
    "\n",
    "    return ir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mdd(returns):\n",
    "    \"\"\"\n",
    "    Calculate the Maximum Drawdown (MDD).\n",
    "\n",
    "    Parameters:\n",
    "    returns (Series or DataFrame): Series or DataFrame containing the returns.\n",
    "\n",
    "    Returns:\n",
    "    float or Series: Maximum Drawdown.\n",
    "    \"\"\"\n",
    "    # Calculate the cumulative returns\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "\n",
    "    # Calculate the maximum value seen up to each point\n",
    "    max_seen = cum_returns.cummax()\n",
    "\n",
    "    # Calculate drawdowns\n",
    "    drawdowns = (cum_returns - max_seen) / max_seen\n",
    "\n",
    "    # Find the maximum drawdown\n",
    "    max_drawdown = drawdowns.min()\n",
    "\n",
    "    return max_drawdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df, column_to_diff='Predictions', column='Predicted Returns'):\n",
    "    \"\"\"\n",
    "    Calculate returns from adjusted close prices.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing adjusted close prices.\n",
    "\n",
    "    Returns:\n",
    "    returns (DataFrame): DataFrame containing the calculated returns in df[column].\n",
    "    \"\"\"\n",
    "    \n",
    "    df[column] = df[column_to_diff].pct_change()\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_buy_and_hold_returns(df, column_to_diff='Predictions', column='Predicted Returns'):\n",
    "    \"\"\"\n",
    "    Calculate returns from adjusted close prices of the bench mark buy and hold strategy.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing adjusted close prices.\n",
    "\n",
    "    Returns:\n",
    "    returns (DataFrame): DataFrame containing the calculated returns in df[column].\n",
    "    \"\"\"\n",
    "    initial_value = df.iloc[0][column_to_diff]\n",
    "    df[column] = (df[column_to_diff] - initial_value)/initial_value\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quantiles(df, column='Predected Return', q1=0.25, q2=0.5, q3=0.75):\n",
    "    Q1 = df[column].quantile(q1)\n",
    "    Q2 = df[column].quantile(q2)  # Median\n",
    "    Q3 = df[column].quantile(q3)\n",
    "\n",
    "    return (Q1, Q2, Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signal(df, return_column='Predicted Returns', signal_column='Predicted Signal', q1=None, q3=None):\n",
    "    if q1 is None or q3 is None:\n",
    "        q1 = df[return_column].quantile(0.25)\n",
    "        q3 = df[return_column].quantile(0.75)\n",
    "\n",
    "    df[signal_column] = 0  # Default signal\n",
    "\n",
    "    df.loc[df[return_column] >= q3, signal_column] = 1\n",
    "    df.loc[df[return_column] <= q1, signal_column] = -1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try \n",
    "sp500 = sp500_data.copy()\n",
    "sp500 = technical_indicators(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laging technical indicators to avoid look_ahead bias\n",
    "#sp500.iloc[:, 6:] = sp500.iloc[:, 6:].apply(lag_technical_indicators, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform features\n",
    "sp500.iloc[:, 6:] = sp500.iloc[:, 6:].apply(feature_transform, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation before choosing features\n",
    "#correlation(sp500, 'Adj Close')\n",
    "#rolling_correlation(sp500, 'Adj Close')\n",
    "\n",
    "# Step 1: Take the most strongly correlated feature and add it to our list of selected features. \n",
    "# Step 2: Take the second correlated feature and check to see if it's closely correlated \n",
    "# (neighboring in the clustermap) to any features already chosen.\n",
    "# If no, add to the list. If yes, discard. \n",
    "# Step 3: Repeat this process until either (1) we've reached the target feature count,\n",
    "# or (2) we've run out strongly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "sp500_d = sp500.copy()\n",
    "sp500_d = drop_nan(sp500_d)\n",
    "sp500_d = sp500_d.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution(sp500_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the correlated data:\n",
    "#features_paiplot(sp500_d)\n",
    "#correlation(sp500_d, 'Adj Close')\n",
    "#rolling_correlation(sp500, 'Adj Close')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is data normal?\", is_normal(sp500_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_counts = count_outliers_iqr_df(sp500_d)\n",
    "print(\"Number of outliers in each column:\")\n",
    "print(outliers_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing features:\n",
    "chosen_features = ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context of regression tasks like this one (since SVR is a Support Vector Machine for regression),\n",
    "# where the target variable is continuous (e.g., log returns), a model score less than 0 typically indicates \n",
    "# that the model is performing poorly and making predictions that are worse than simply using the mean or another\n",
    "# basic statistical measure as the prediction for all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arima Model without Time Sieries Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "data = sp500.copy()\n",
    "data = drop_nan(data)\n",
    "data = data.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA to select optimal ARIMA parameters\n",
    "model = auto_arima(data['Adj Close'], seasonal=False, trace=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Arima Model is ARIMA(2,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data Splitted 80% - 20% - Walk Forward on test (the predictions added to the train)\n",
    "The Method of WF as it was used for other Models WAS NOT USED HERE!!!\n",
    "The Predictions were really bad when it was tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ex_df = data['Adj Close']\n",
    "# Define the ARIMA model\n",
    "def arima_forecast(history):\n",
    "    # Fit the model\n",
    "    model = ARIMA(history, order=(2,1,2))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Make the prediction\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    return yhat\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = dataset_ex_df.values\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "\n",
    "# Walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    # Generate a prediction\n",
    "    yhat = arima_forecast(history)\n",
    "    predictions.append(yhat)\n",
    "    # Add the predicted value to the training set\n",
    "    obs = test[t]\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(dataset_ex_df.iloc[size:].index, test, label='Real')\n",
    "plt.plot(dataset_ex_df.iloc[size:].index, predictions, color='red', label='Predicted')\n",
    "plt.title('ARIMA Predictions vs Actual Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "mae = mean_absolute_error(test,predictions)\n",
    "mse = mean_squared_error(test,predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(test,predictions)\n",
    "mape = mean_absolute_percentage_error(test,predictions)\n",
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(test,predictions)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}\\nMean Squared Error: {mse}\\nRoot Mean Squared Error: {rmse}\\nR-squared: {r2}')\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "# Create traces\n",
    "trace1 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=test, mode='lines', name='Adjusted Close')\n",
    "trace2 = go.Scatter(x=dataset_ex_df.iloc[size:].index, y=predictions, mode='lines', name='Adj_Close_diff')\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces to figure\n",
    "fig.add_trace(trace1)\n",
    "fig.add_trace(trace2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='ARIMA Predictions vs Actual Values',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='USD',\n",
    "                  hovermode='x unified')\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class For All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModels:\n",
    "    def __init__(self, k=5, svr_kernel='rbf', C=1.0, gamma=0.1, degree=3, rf_n_estimators=100, rf_max_features=4, \n",
    "                 rf_criterion = 'squared_error', gbm_n_estimators=100, gbm_criterion='squared_error',\n",
    "                 gbm_loss='squared_error', gbm_n_features='sqrt', rf_warm_start=False):\n",
    "\n",
    "        self.k = k\n",
    "        self.svr_kernel = svr_kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.C = C\n",
    "        self.rf_n_estimators = rf_n_estimators\n",
    "        self.rf_max_features = rf_max_features\n",
    "        self.rf_criterion = rf_criterion\n",
    "        self.rf_warm_start=rf_warm_start\n",
    "        self.gbm_n_estimators = gbm_n_estimators\n",
    "        self.gbm_criterion = gbm_criterion\n",
    "        self.gbm_loss = gbm_loss\n",
    "        self.gbm_n_features = gbm_n_features\n",
    "\n",
    "        # Initialize models\n",
    "        self.knn_model = KNeighborsRegressor(n_neighbors=self.k)\n",
    "        self.svr_model = SVR(kernel=self.svr_kernel, gamma=self.gamma, degree=self.degree, C=self.C)\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=self.rf_n_estimators,\n",
    "                         max_features=self.rf_max_features, criterion=self.rf_criterion, bootstrap=False, warm_start=self.rf_warm_start)\n",
    "        self.gbm_model = GradientBoostingRegressor(n_estimators=self.gbm_n_estimators, \n",
    "                                                   criterion=self.gbm_criterion, loss=self.gbm_loss\n",
    "                                                   ,max_features=self.gbm_n_features)\n",
    "        self.lr_model = LinearRegression()\n",
    "\n",
    "    def knn_fit(self, dfX, vY):\n",
    "        self.knn_model.fit(dfX, vY)\n",
    "\n",
    "    def svr_fit(self, dfX, vY):\n",
    "        self.svr_model.fit(dfX, vY)\n",
    "\n",
    "    def rf_fit(self, dfX, vY):\n",
    "        self.rf_model.fit(dfX, vY)\n",
    "\n",
    "    def gbm_fit(self, dfX, vY):\n",
    "        self.gbm_model.fit(dfX, vY)\n",
    "    \n",
    "    def lr_fit(self, dfX, vY):\n",
    "        self.lr_model.fit(dfX, vY)\n",
    "\n",
    "    def knn_predict(self, dfX):\n",
    "        return self.knn_model.predict(dfX)\n",
    "\n",
    "    def svr_predict(self, dfX):\n",
    "        return self.svr_model.predict(dfX)\n",
    "\n",
    "    def rf_predict(self, dfX):\n",
    "        return self.rf_model.predict(dfX)\n",
    "\n",
    "    def gbm_predict(self, dfX):\n",
    "        return self.gbm_model.predict(dfX)\n",
    "    \n",
    "    def lr_predict(self, dfX):\n",
    "        return self.lr_model.predict(dfX)\n",
    "\n",
    "    def knn_score(self, dfX, vY):\n",
    "        return self.knn_model.score(dfX, vY)\n",
    "\n",
    "    def svr_score(self, dfX, vY):\n",
    "        return self.svr_model.score(dfX, vY)\n",
    "\n",
    "    def rf_score(self, dfX, vY):\n",
    "        return self.rf_model.score(dfX, vY)\n",
    "\n",
    "    def gbm_score(self, dfX, vY):\n",
    "        return self.gbm_model.score(dfX, vY)\n",
    "    \n",
    "    def lr_score(self, dfX, vY):\n",
    "        return self.lr_model.score(dfX, vY)\n",
    "\n",
    "    def knn_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        \n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.knn_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.knn_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def svr_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.svr_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.svr_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def rf_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.rf_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.rf_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "\n",
    "    def gbm_train(self, splits:tuple, col_drop, param_grid:dict, n_splits=3):\n",
    "        mse_scores = []\n",
    "        all_pred = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            # Initialize Gradient Boosting model\n",
    "            model = GradientBoostingRegressor()\n",
    "            # Define time series split for cross-validation\n",
    "            tscv = TimeSeriesSplit(n_splits)\n",
    "            \n",
    "            # Perform GridSearchCV for hyperparameter optimization and Pass time series split to GridSearchCV\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=None)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Get the best model from GridSearchCV\n",
    "            best_model = grid_search.best_estimator_\n",
    "            # Get the best model from GridSearchCV\n",
    "            best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            y_test = out_of_sample[col_drop]\n",
    "\n",
    "            # Make predictions on the out-of-sample data\n",
    "            predictions = best_model.predict(X_test)\n",
    "            all_pred.append(predictions)\n",
    "                \n",
    "            # Calculate Mean Squared Error (MSE) as the performance metric\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "            return (all_pred, mse_scores, best_hyperparameters)# Return both predictions and evaluations\n",
    "    \n",
    "    def lr_train(self, splits, col_drop):\n",
    "        predictions = []\n",
    "        for i, (in_sample, out_of_sample) in enumerate(splits):\n",
    "            X_train = in_sample.drop(columns=[col_drop])\n",
    "            y_train = in_sample[col_drop]\n",
    "            self.lr_model.fit(X_train, y_train)\n",
    "\n",
    "            X_test = out_of_sample.drop(columns=[col_drop])\n",
    "            prediction = self.lr_model.predict(X_test)\n",
    "            predictions.extend(prediction)\n",
    "        return (predictions)# Return both predictions and evaluations\n",
    "    \n",
    "    def evaluate(self, y_true, y_pred):\n",
    "        self.mae = mean_absolute_error(y_true, y_pred)\n",
    "        self.mse = mean_squared_error(y_true, y_pred)\n",
    "        self.rmse = np.sqrt(self.mse)\n",
    "        self.r2 = r2_score(y_true, y_pred)\n",
    "        self.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        return (self.mae, self.mse, self.rmse, self.r2, self.mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to perform Optemization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "sp500_d = sp500.copy()\n",
    "sp500_d = drop_nan(sp500_d)\n",
    "sp500_d = sp500_d.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "sp500_d = truncate_before_wf(sp500_d, 320, 80)\n",
    "splits = walk_forward_validation(sp500_d, 320, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search SVR Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lC      = [1, 10, 50, 100]\n",
    "lKernel = ['poly', 'rbf','linear']\n",
    "lgamma      = ['scale', 'auto', 0.1, 0.01, 0.001]\n",
    "ldegree = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(lKernel) * len(lC) * len(lgamma) * len(ldegree)\n",
    "dData   = {'kernel': [], 'C': [], 'gamma':[], 'degree':[]}\n",
    "\n",
    "for ii, kernel in enumerate(lKernel):\n",
    "    for jj, paramC in enumerate(lC):\n",
    "        for kk, gamma in enumerate(lgamma):\n",
    "            for cc, degree in enumerate(ldegree):\n",
    "                dData['kernel'].append(kernel)\n",
    "                dData['C'].append(paramC)\n",
    "                dData['gamma'].append(gamma)\n",
    "                dData['degree'].append(degree)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(svr_kernel='rbf', C=1.0, gamma=0.1, degree=3)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for ii in range(numComb):\n",
    "    kernel    = dfModelScore.loc[ii, 'kernel']\n",
    "    paramC          = dfModelScore.loc[ii, 'C']\n",
    "    gamma          = dfModelScore.loc[ii, 'gamma']\n",
    "    degree          = dfModelScore.loc[ii, 'degree']\n",
    "\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "\n",
    "    Model = TimeSeriesModels(svr_kernel=kernel, C=paramC, gamma=gamma, degree=degree)\n",
    "    predictions = Model.svr_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[ii, 'MAE'] = mae\n",
    "    dfModelScore.loc[ii, 'MSE'] = mse\n",
    "    dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[ii, 'R2'] = r2\n",
    "    dfModelScore.loc[ii, 'MAPE'] = mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Best Model Score or SVR Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['RMSE'].idxmin()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['kernel', 'C', 'gamma', 'degree']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best SVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(svr_kernel='linear', C=100, gamma='scale', degree=3)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "predictions = Model.svr_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "\n",
    "# Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "dfModelScore.loc['MAE'] = mae\n",
    "dfModelScore.loc['MSE'] = mse\n",
    "dfModelScore.loc['RMSE'] = rmse\n",
    "dfModelScore.loc['R2'] = r2\n",
    "dfModelScore.loc['MAPE'] = mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dData   = {'K': []}\n",
    "\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for kk in range(10):\n",
    "    print(f'Processing model {kk + 1:03d} out of {10}')\n",
    "\n",
    "    Model = TimeSeriesModels(k=kk + 1)\n",
    "    predictions = Model.knn_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY[-80:], predictions[-80:])\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[kk, 'k'] = kk + 1\n",
    "    dfModelScore.loc[kk, 'MAE'] = mae\n",
    "    dfModelScore.loc[kk, 'MSE'] = mse\n",
    "    dfModelScore.loc[kk, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[kk, 'R2'] = r2\n",
    "    dfModelScore.loc[kk, 'MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'RMSE', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['MeanScore'].idxmin()\n",
    "\n",
    "# Find the index of the model with the higher r2\n",
    "best_model_index_r2 = dfModelScore['R2'].idxmax()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['k']]\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params_r2 = dfModelScore.loc[best_model_index_r2, ['k']]\n",
    "\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters - lowest mean of (MAE, RMSE, MAPE):\")\n",
    "print(best_model_params)\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters - Higher R2:\")\n",
    "print(best_model_params_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(k=1)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "predictions = Model.knn_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegressor - no hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "\n",
    "predictions = Model.lr_train(splits, 'Adj Close')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "\n",
    "# Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "dfModelScore.loc['MAE'] = mae\n",
    "dfModelScore.loc['MSE'] = mse\n",
    "dfModelScore.loc['RMSE'] = rmse\n",
    "dfModelScore.loc['R2'] = r2\n",
    "dfModelScore.loc['MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_features = ['sqrt', 'log2', 4, 5, 6, 7]\n",
    "#criterion = ['squared_error', 'MAE']\n",
    "criterion = ['squared_error']\n",
    "n_estimators = [100, 150, 200] #number of threes in the forest (100 default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(m_features) * len(criterion) * len(n_estimators)\n",
    "dData   = {'max_features': [], 'criterion': [], 'n_estimators':[]}\n",
    "\n",
    "for ii, feature in enumerate(m_features):\n",
    "    for jj, cri in enumerate(criterion):\n",
    "        for kk, est in enumerate(n_estimators):\n",
    "            dData['max_features'].append(feature)\n",
    "            dData['criterion'].append(cri)\n",
    "            dData['n_estimators'].append(est)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "for ii in range(numComb):\n",
    "    rf_n_feature = dfModelScore.loc[ii, 'max_features']\n",
    "    rf_criterion = dfModelScore.loc[ii, 'criterion']\n",
    "    rf_n_est = dfModelScore.loc[ii, 'n_estimators']\n",
    "\n",
    "    print(f'Processing model {ii + 1:03d} out of {numComb}')\n",
    "    Model = TimeSeriesModels(rf_n_estimators=rf_n_est, rf_max_features=rf_n_feature ,rf_criterion = rf_criterion, rf_warm_start=True)\n",
    "    predictions = Model.rf_train(splits, 'Adj Close')\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "    # Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "    dfModelScore.loc[ii, 'MAE'] = mae\n",
    "    dfModelScore.loc[ii, 'MSE'] = mse\n",
    "    dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "    dfModelScore.loc[ii, 'R2'] = r2\n",
    "    dfModelScore.loc[ii, 'MAPE'] = mape\n",
    "\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "best_model_index = dfModelScore['RMSE'].idxmin()\n",
    "#best_model_index = dfModelScore['R2'].idxmax()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['max_features', 'criterion', 'n_estimators']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfModelScore.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the best Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(rf_n_estimators=150, rf_max_features=7)\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "predictions = Model.rf_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_loss = ['squared_error']\n",
    "#L_loss = ['squared_error', 'absolute_error', 'huber']\n",
    "L_criterion = ['squared_error']\n",
    "#L_criterion = ['friedman_mse', 'squared_error']\n",
    "L_n_estimators = [100, 150, 200] #number of threes in the forest (100 default)\n",
    "L_m_features = ['sqrt', 'log2', 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Frame\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Calculate the number of combinations.\n",
    "# 2. Create a nested loop to create the combinations between the parameters.\n",
    "# 3. Store the combinations as the columns of a data frame.\n",
    "\n",
    "# For Advanced Python users: Use iteration tools for create the cartesian product\n",
    "numComb = len(L_loss) * len(L_criterion) * len(L_n_estimators) *len(L_m_features)\n",
    "dData   = {'loss': [], 'criterion': [], 'n_estimators':[], 'm_features':[]}\n",
    "\n",
    "for ii, lss in enumerate(L_loss):\n",
    "    for jj, cri in enumerate(L_criterion):\n",
    "        for kk, est in enumerate(L_n_estimators):\n",
    "            for kk, feature in enumerate(L_m_features):\n",
    "                dData['loss'].append(lss)\n",
    "                dData['criterion'].append(cri)\n",
    "                dData['n_estimators'].append(est)\n",
    "                dData['m_features'].append(feature)\n",
    "#===============================================================#\n",
    "\n",
    "dfModelScore = pd.DataFrame(data = dData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels()\n",
    "\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "vY = sp500_d['Adj Close'].values\n",
    "vY = vY[index_dropped:]\n",
    "predictions, eval, bestmodelparams= Model.gbm_train(splits, 'Adj Close', param_grid)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae, mse, rmse, r2, mape = Model.evaluate(vY, predictions)\n",
    "    \n",
    "# Update the 'R2' column in dfModelScore with the calculated R2 score\n",
    "dfModelScore.loc[ii, 'MAE'] = mae\n",
    "dfModelScore.loc[ii, 'MSE'] = mse\n",
    "dfModelScore.loc[ii, 'RMSE'] = rmse\n",
    "dfModelScore.loc[ii, 'R2'] = r2\n",
    "dfModelScore.loc[ii, 'MAPE'] = mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of MAE, MSE, RMSE, and MAPE for each model\n",
    "dfModelScore['MeanScore'] = dfModelScore[['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']].mean(axis=1)\n",
    "\n",
    "# Find the index of the model with the lowest mean score\n",
    "#best_model_index = dfModelScore['MeanScore'].idxmin()\n",
    "#best_model_index = dfModelScore['R2'].idxmax()\n",
    "best_model_index = dfModelScore['RMSE'].idxmin()\n",
    "\n",
    "# Get the parameters of the best model\n",
    "best_model_params = dfModelScore.loc[best_model_index, ['loss', 'criterion', 'n_estimators', 'm_features']]\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = TimeSeriesModels(gbm_n_estimators=200)\n",
    "# Initialize an empty list to store predictions & adjust the df index (should cut the first unpredected 200)\n",
    "index_dropped = 320\n",
    "predictions, _ = Model.gbm_train(splits, 'Adj Close')\n",
    "\n",
    "sp500_d_includes_results = sp500_d.copy()\n",
    "sp500_d_includes_results = sp500_d_includes_results.iloc[index_dropped:, :]\n",
    "sp500_d_includes_results['Predictions'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange the evaluations from each iteration\n",
    "eval = [eval[i:i+5] for i in range(0, len(eval), 5)]\n",
    "c_names = ['MAE', 'MSE', 'RMSE', 'R2', 'MAPE']\n",
    "eval = pd.DataFrame(eval, columns=c_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the evaluations for each iteration of the WF\n",
    "for column in eval.columns:\n",
    "    plt.figure()  # Create a new figure for each plot\n",
    "    plt.plot(eval.index, eval[column], label=column)  # Plot the column\n",
    "    plt.xlabel('Split of Walk Forward Analysis')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title(f'Plot of {column}')  # Add title with column name\n",
    "    plt.legend()  # Show legend\n",
    "    plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "mae = mean_absolute_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "mse = mean_squared_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "mape = mean_absolute_percentage_error(sp500_d_includes_results['Adj Close'], sp500_d_includes_results['Predictions'] )\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predictions'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=sp500_d_includes_results.index,  # Assuming the index represents x-axis values\n",
    "    y=sp500_d_includes_results[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sp500_d_includes_results.index,  # Assuming the index represents x-axis values\n",
    "    y=sp500_d_includes_results[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - log_return Vs. Predictions on train dataset - WF 200,20',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Predected values to returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = sp500_d_includes_results.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = calculate_returns(analyze, 'Adj Close', 'Returns')\n",
    "analyze = calculate_returns(analyze, 'Predictions', 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Returns Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution(analyze, 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_distribution(analyze, 'Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2, q3 = calculate_quantiles(analyze, 'Predicted Returns')\n",
    "\n",
    "print(\"Q1 (25th percentile):\\n\", q1)\n",
    "print(\"\\nQ2 (50th percentile - Median):\\n\", q2)\n",
    "print(\"\\nQ3 (75th percentile):\\n\", q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Trading Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(analyze, 'Returns', 'Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(analyze, 'Predicted Returns', 'Predicted Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'Returns'\n",
    "feature2 = 'Predicted Returns'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line plot for feature1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=analyze.index,  # Assuming the index represents x-axis values\n",
    "    y=analyze[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add line plot for feature2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=analyze.index,  # Assuming the index represents x-axis values\n",
    "    y=analyze[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Calculate quartiles for feature1\n",
    "q1_feature1 = analyze[feature1].quantile(0.25)\n",
    "q3_feature1 = analyze[feature1].quantile(0.75)\n",
    "\n",
    "# Calculate quartiles for feature2\n",
    "q1_feature2 = analyze[feature2].quantile(0.25)\n",
    "q3_feature2 = analyze[feature2].quantile(0.75)\n",
    "\n",
    "# Add horizontal lines for quartiles for feature1\n",
    "fig.add_hline(y=q1_feature1, line_dash=\"dash\", line_color=\"green\", annotation_text=f'{feature1} Q1: {q1_feature1}', annotation_position=\"bottom right\")\n",
    "fig.add_hline(y=q3_feature1, line_dash=\"dash\", line_color=\"orange\", annotation_text=f'{feature1} Q3: {q3_feature1}', annotation_position=\"top right\")\n",
    "\n",
    "# Add horizontal lines for quartiles for feature2\n",
    "fig.add_hline(y=q1_feature2, line_dash=\"dash\", line_color=\"black\", annotation_text=f'{feature2} Q1: {q1_feature2}', annotation_position=\"bottom left\")\n",
    "fig.add_hline(y=q3_feature2, line_dash=\"dash\", line_color=\"white\", annotation_text=f'{feature2} Q3: {q3_feature2}', annotation_position=\"top left\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - Predicted Returns Vs. Returns - WF 320,80',\n",
    "    xaxis=dict(title='Time Index'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Returns'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate bench mark buy and hold returns\n",
    "analyze = calculate_buy_and_hold_returns(analyze, 'Adj Close', 'Returns BH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharpe Ratio / Information Ratio / Maximum Drowdown: returns vs predected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_true = calculate_sharpe_ratio(analyze['Returns'])\n",
    "sharp_pred = calculate_sharpe_ratio(analyze['Predicted Returns'])\n",
    "sharp_BH = calculate_sharpe_ratio(analyze['Returns BH'])\n",
    "ir_true = calculate_ir(analyze['Returns'],  analyze['Returns BH'])\n",
    "ir_pred = calculate_ir(analyze['Predicted Returns'],  analyze['Returns BH'])\n",
    "ir_BH = calculate_ir(analyze['Returns BH'],  analyze['Returns BH'])\n",
    "mdd_true = calculate_mdd(analyze['Returns'])\n",
    "mdd_pred = calculate_mdd(analyze['Predicted Returns'])\n",
    "mdd_BH = calculate_mdd(analyze['Returns BH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sharp_true, sharp_pred, sharp_BH, ir_true, ir_pred, ir_BH, mdd_true, mdd_pred, mdd_BH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_data_n = yf.download('^GSPC', start='2021-01-02', end='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try \n",
    "dfX = sp500_data_n.copy()\n",
    "dfX = technical_indicators(dfX)\n",
    "dfX.iloc[:, 6:] = dfX.iloc[:, 6:].apply(feature_transform, axis=0)\n",
    "dfX = drop_nan(dfX)\n",
    "dfX = dfX.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dfX = dfX.drop(columns='Adj Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfX['Predicted Adj Close'] = Model.knn_predict(feat_dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predicted Adj Close'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - log_return Vs. Predictions',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = calculate_returns(dfX, 'Adj Close', 'Returns')\n",
    "dfX = calculate_returns(dfX, 'Predicted Adj Close', 'Predicted Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_signal(dfX, 'Predicted Returns', 'Predicted Signal')\n",
    "calculate_signal(dfX, 'Returns', 'Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'Returns'\n",
    "feature2 = 'Predicted Returns'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line plot for feature1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add line plot for feature2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Calculate quartiles for feature1\n",
    "q1_feature1 = dfX[feature1].quantile(0.25)\n",
    "q3_feature1 = dfX[feature1].quantile(0.75)\n",
    "\n",
    "# Calculate quartiles for feature2\n",
    "q1_feature2 = dfX[feature2].quantile(0.25)\n",
    "q3_feature2 = dfX[feature2].quantile(0.75)\n",
    "\n",
    "# Add horizontal lines for quartiles for feature1\n",
    "fig.add_hline(y=q1_feature1, line_dash=\"dash\", line_color=\"green\", annotation_text=f'{feature1} Q1: {q1_feature1}', annotation_position=\"bottom right\")\n",
    "fig.add_hline(y=q3_feature1, line_dash=\"dash\", line_color=\"orange\", annotation_text=f'{feature1} Q3: {q3_feature1}', annotation_position=\"top right\")\n",
    "\n",
    "# Add horizontal lines for quartiles for feature2\n",
    "fig.add_hline(y=q1_feature2, line_dash=\"dash\", line_color=\"black\", annotation_text=f'{feature2} Q1: {q1_feature2}', annotation_position=\"bottom left\")\n",
    "fig.add_hline(y=q3_feature2, line_dash=\"dash\", line_color=\"white\", annotation_text=f'{feature2} Q3: {q3_feature2}', annotation_position=\"top left\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - Predicted Returns Vs. Returns - WF 600,200',\n",
    "    xaxis=dict(title='Time Index'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Returns'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "mse = mean_squared_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "mape = mean_absolute_percentage_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error: 1556.229927897793\n",
    "Mean Squared Error: 2452416.250634323\n",
    "Root Mean Squared Error: 1566.0192369936976\n",
    "R-squared: -26.833884620726344\n",
    "Mean Absolute Percentage Error (MAPE): 0.36939950873652055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(dfX['Signal'], dfX['Predicted Signal'])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code for WF optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data from na values & pick the columns\n",
    "sp500_WFO = sp500.copy()\n",
    "sp500_WFO = drop_nan(sp500_WFO)\n",
    "sp500_WFO = sp500_WFO.loc[:, ['SMA','WMA', 'MACD', 'RSI', '%K_fast', '%D_fast', '%D_slow', 'Bollinger Diff',\n",
    "                    'WPR', 'OBV', 'ROC', 'ATR', 'MFI', 'Chaikin_Oscillator', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_WFO = truncate_before_wf(sp500_WFO, 1300, 325)\n",
    "splits = walk_forward_validation(sp500_WFO, 1300, 325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_gradient_boosting(train_df, test_df, column = 'target', grid_params):\n",
    "    \"\"\"\n",
    "    Train and test Gradient Boosting models with different parameter combinations.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: DataFrame containing training features and target.\n",
    "    - test_df: DataFrame containing testing features and target.\n",
    "    - grid_params: List of dictionaries, each containing a set of parameters for Gradient Boosting.\n",
    "    \n",
    "    Returns:\n",
    "    - best_params: Dictionary of the best parameters found.\n",
    "    - train_score: Train score corresponding to the best parameters.\n",
    "    - test_score: Test score corresponding to the best parameters.\n",
    "    \"\"\"\n",
    "    best_params = None\n",
    "    best_train_score = float('-inf')\n",
    "    best_test_score = float('-inf')\n",
    "    \n",
    "    # Extract features and target from the DataFrames\n",
    "    X_train = train_df.drop(columns=[column])\n",
    "    y_train = train_df[column]\n",
    "    X_test = test_df.drop(columns=[column])\n",
    "    y_test = test_df[column]\n",
    "    \n",
    "    # Iterate over each set of parameters\n",
    "    for params in grid_params:\n",
    "        # Create a Gradient Boosting model with the current parameters\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model on the training data\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_score = mean_squared_error(y_train, train_pred)\n",
    "        \n",
    "        # Evaluate the model on the testing data\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_score = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "        # Update best parameters and scores if necessary\n",
    "        if test_score > best_test_score:\n",
    "            best_params = params\n",
    "            best_train_score = train_score\n",
    "            best_test_score = test_score\n",
    "    \n",
    "    return best_params, best_train_score, best_test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Assume 'data' contains your historical data, including 'date', 'adjusted_close', and any features\n",
    "\n",
    "# Define the number of time windows and their sizes\n",
    "num_windows = 12\n",
    "window_size = 1625\n",
    "\n",
    "# Initialize lists to store performance metrics\n",
    "mse_scores = []\n",
    "all_pred = []\n",
    "\n",
    "# Perform Walk Forward Analysis\n",
    "for i in range(num_windows - 1):  # Iterate through each time window\n",
    "    # Define start and end indices for the current time window\n",
    "    start_index = i * window_size\n",
    "    end_index = (i + 1) * window_size\n",
    "\n",
    "    # Extract in-sample and out-of-sample data\n",
    "    in_sample_data = sp500_WFO[start_index:end_index]\n",
    "    out_of_sample_data = sp500_WFO[end_index:end_index + window_size]\n",
    "\n",
    "    # Extract features and target variable from the data\n",
    "    X_train = in_sample_data.drop(columns='Adj Close')\n",
    "    y_train = in_sample_data['Adj Close']\n",
    "    X_test = out_of_sample_data.drop(columns='Adj Close')\n",
    "    y_test = out_of_sample_data['Adj Close']\n",
    "\n",
    "    # Define hyperparameters to search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    # Initialize Gradient Boosting model\n",
    "    model = GradientBoostingRegressor()\n",
    "    \n",
    "    # Define time series split for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # Perform GridSearchCV for hyperparameter optimization and Pass time series split to GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=tscv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model from GridSearchCV\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    ## Get the best model from GridSearchCV\n",
    "    best_hyperparameters = grid_search.best_params_\n",
    "\n",
    "    # Make predictions on the out-of-sample data\n",
    "    predictions = best_model.predict(X_test)\n",
    "    all_pred.append(predictions)\n",
    "    # Calculate Mean Squared Error (MSE) as the performance metric\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Print MSE scores for each time window\n",
    "print(\"MSE Scores for Each Time Window:\", mse_scores)\n",
    "\n",
    "print(\"Best Hyper Params:\", best_hyperparameters)\n",
    "\n",
    "# Calculate and print average MSE score\n",
    "average_mse = np.mean(mse_scores)\n",
    "print(\"Average MSE:\", average_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(all_pred)\n",
    "\n",
    "all_pred_f = np.concatenate(arr, axis=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(all_pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_WFO['Predictions'] = np.nan\n",
    "sp500_WFO.iloc[:4367, sp500_WFO.columns.get_loc('Predictions')] = all_pred_f\n",
    "sp500_WFO = drop_nan(sp500_WFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs. actual values\n",
    "plt.scatter(sp500_WFO['Adj Close'], sp500_WFO['Predictions'] )\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "mae = mean_absolute_error(sp500_WFO['Adj Close'], sp500_WFO['Predictions'] )\n",
    "mse = mean_squared_error(sp500_WFO['Adj Close'], sp500_WFO['Predictions'] )\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(sp500_WFO['Adj Close'], sp500_WFO['Predictions'] )\n",
    "mape = mean_absolute_percentage_error(sp500_WFO['Adj Close'], sp500_WFO['Predictions'] )\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX['Predicted Adj Close'] = best_model.predict(feat_dfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame containing the data\n",
    "# Replace 'feature1' and 'feature2' with the names of the features you want to plot\n",
    "feature1 = 'Adj Close'\n",
    "feature2 = 'Predicted Adj Close'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add another line plot for the second feature\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - log_return Vs. Predictions',\n",
    "    xaxis=dict(title='X Axis'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Y Axis'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = calculate_returns(dfX, 'Predicted Adj Close', 'predicted_returns')\n",
    "dfX = calculate_returns(dfX, 'Adj Close', 'true_returns')\n",
    "dfX = calculate_signal(dfX, 'Predicted Adj Close', 'predicted_signals')\n",
    "dfX = calculate_signal(dfX, 'Adj Close', 'true_signals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "mse = mean_squared_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "mape = mean_absolute_percentage_error(dfX['Adj Close'], dfX['Predicted Adj Close'] )\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'true_returns'\n",
    "feature2 = 'predicted_returns'\n",
    "\n",
    "# Create line plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line plot for feature1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature1],\n",
    "    mode='lines',\n",
    "    name=feature1\n",
    "))\n",
    "\n",
    "# Add line plot for feature2\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=dfX.index,  # Assuming the index represents x-axis values\n",
    "    y=dfX[feature2],\n",
    "    mode='lines',\n",
    "    name=feature2\n",
    "))\n",
    "\n",
    "# Calculate quartiles for feature1\n",
    "q1_feature1 = dfX[feature1].quantile(0.25)\n",
    "q3_feature1 = dfX[feature1].quantile(0.75)\n",
    "\n",
    "# Calculate quartiles for feature2\n",
    "q1_feature2 = dfX[feature2].quantile(0.25)\n",
    "q3_feature2 = dfX[feature2].quantile(0.75)\n",
    "\n",
    "# Add horizontal lines for quartiles for feature1\n",
    "fig.add_hline(y=q1_feature1, line_dash=\"dash\", line_color=\"green\", annotation_text=f'{feature1} Q1: {q1_feature1}', annotation_position=\"bottom right\")\n",
    "fig.add_hline(y=q3_feature1, line_dash=\"dash\", line_color=\"orange\", annotation_text=f'{feature1} Q3: {q3_feature1}', annotation_position=\"top right\")\n",
    "\n",
    "# Add horizontal lines for quartiles for feature2\n",
    "fig.add_hline(y=q1_feature2, line_dash=\"dash\", line_color=\"black\", annotation_text=f'{feature2} Q1: {q1_feature2}', annotation_position=\"bottom left\")\n",
    "fig.add_hline(y=q3_feature2, line_dash=\"dash\", line_color=\"white\", annotation_text=f'{feature2} Q3: {q3_feature2}', annotation_position=\"top left\")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Model - Predicted Returns Vs. Returns - WF 200,20',\n",
    "    xaxis=dict(title='Time Index'),  # Customize x-axis label\n",
    "    yaxis=dict(title='Returns'),  # Customize y-axis label\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = calculate_accuracy(dfX['true_signals'], dfX['predicted_signals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
